{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC sampling using the emcee package\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of Markov Chain Monte Carlo (MCMC) algorithms is to approximate the posterior distribution of your model parameters by random sampling in a probabilistic space. For most readers this sentence was probably not very helpful so here we'll start straight with and example but you should read the more detailed mathematical approaches of the method [here](https://www.pas.rochester.edu/~sybenzvi/courses/phy403/2015s/p403_17_mcmc.pdf) and [here](https://github.com/jakevdp/BayesianAstronomy/blob/master/03-Bayesian-Modeling-With-MCMC.ipynb).\n",
    "\n",
    "### How does it work ?\n",
    "\n",
    "The idea is that we use a number of walkers that will sample the posterior distribution (i.e. sample the Likelihood profile).\n",
    "\n",
    "The goal is to produce a \"chain\", i.e. a list of $\\theta$ values, where each $\\theta$ is a vector of parameters for your model.<br>\n",
    "If you start far away from the truth value, the chain will take some time to converge until it reaches a stationary state. Once it has reached this stage, each successive elements of the chain are samples of the target posterior distribution.<br>\n",
    "This means that, once we have obtained the chain of samples, we have everything we need. We can compute the  distribution of each parameter by simply approximating it with the histogram of the samples projected into the parameter space. This will provide the errors and correlations between parameters.\n",
    "\n",
    "\n",
    "Now let's try to put a picture on the ideas described above. With this notebook, we have simulated and carried out a MCMC analysis for a source with the following parameters:<br>\n",
    "$Index=2.0$, $Norm=5\\times10^{-12}$ cm$^{-2}$ s$^{-1}$ TeV$^{-1}$, $Lambda =(1/Ecut) = 0.02$ TeV$^{-1}$ (50 TeV) for 20 hours.\n",
    "\n",
    "The results that you can get from a MCMC analysis will look like this :\n",
    "\n",
    "<img src=\"gammapy_mcmc.png\" width=\"800\">\n",
    "\n",
    "On the first two top panels, we show the pseudo-random walk of one walker from an offset starting value to see it evolve to a better solution.\n",
    "In the bottom right panel, we show the trace of each 16 walkers for 500 runs (the chain described previsouly). For the first 100 runs, the parameter evolve towards a solution (can be viewed as a fitting step). Then they explore the local minimum for 400 runs which will be used to estimate the parameters correlations and errors.\n",
    "The choice of the Nburn value (when walkers have reached a stationary stage) can be done by eye but you can also look at the autocorrelation time.\n",
    "\n",
    "### Why should I use it ?\n",
    "\n",
    "When it comes to evaluate errors and investigate parameter correlation, one typically estimate the Likelihood in a gridded search (2D Likelihood profiles). Each point of the grid implies a new model fitting. If we use 10 steps for each parameters, we will need to carry out 100 fitting procedures. \n",
    "\n",
    "Now let's say that I have a model with $N$ parameters, we need to carry out that gridded analysis $N*(N-1)$ times. \n",
    "So for 5 free parameters you need 20 gridded search, resulting in 2000 individual fit. \n",
    "Clearly this strategy doesn't scale well to high-dimensional models.\n",
    "\n",
    "Just for fun: if each fit procedure takes 10s, we're talking about 5h of computing time to estimate the correlation plots. \n",
    "\n",
    "There are many MCMC packages in the python ecosystem but here we will focus on [emcee](https://emcee.readthedocs.io), a lightweight Python package. A description is provided here : [Foreman-Mackey, Hogg, Lang & Goodman (2012)](https://arxiv.org/abs/1202.3665)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from gammapy.irf import load_irf_dict_from_file\n",
    "from gammapy.maps import WcsGeom, MapAxis\n",
    "from gammapy.modeling.models import (\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    PowerLawSpectralModel,\n",
    "    GaussianSpatialModel,\n",
    "    SkyModel,\n",
    "    Models,\n",
    "    FoVBackgroundModel,\n",
    "    GaussianPrior,\n",
    "    UniformPrior,\n",
    ")\n",
    "from gammapy.datasets import MapDataset\n",
    "from gammapy.makers import MapDatasetMaker\n",
    "from gammapy.data import Observation\n",
    "\n",
    "from gammapy.modeling import Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate an observation\n",
    "\n",
    "Here we will start by simulating an observation using the `simulate_dataset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irfs = load_irf_dict_from_file(\n",
    "    \"$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\"\n",
    ")\n",
    "\n",
    "observation = Observation.create(\n",
    "    pointing=SkyCoord(0 * u.deg, 0 * u.deg, frame=\"galactic\"),\n",
    "    livetime=20 * u.h,\n",
    "    irfs=irfs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map geometry\n",
    "axis = MapAxis.from_edges(\n",
    "    np.logspace(-1, 2, 15), unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0), binsz=0.05, width=(2, 2), frame=\"galactic\", axes=[axis]\n",
    ")\n",
    "\n",
    "empty_dataset = MapDataset.create(geom=geom, name=\"dataset-mcmc\")\n",
    "maker = MapDatasetMaker(selection=[\"background\", \"edisp\", \"psf\", \"exposure\"])\n",
    "dataset = maker.run(empty_dataset, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sky model to simulate the data\n",
    "spatial_model = GaussianSpatialModel(\n",
    "    lon_0=\"0 deg\", lat_0=\"0 deg\", sigma=\"0.2 deg\", frame=\"galactic\"\n",
    ")\n",
    "\n",
    "spectral_model = ExpCutoffPowerLawSpectralModel(\n",
    "    index=2,\n",
    "    amplitude=\"3e-12 cm-2 s-1 TeV-1\",\n",
    "    reference=\"1 TeV\",\n",
    "    lambda_=\"0.05 TeV-1\",\n",
    ")\n",
    "\n",
    "\n",
    "sky_model_simu = SkyModel(\n",
    "    spatial_model=spatial_model, spectral_model=spectral_model, name=\"source\"\n",
    ")\n",
    "\n",
    "bkg_model = FoVBackgroundModel(dataset_name=dataset.name)\n",
    "models = Models([sky_model_simu, bkg_model])\n",
    "models_true = models.copy()  # comparison later between true and fitted values\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.models = models\n",
    "dataset.fake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.counts.sum_over_axes().plot(add_cbar=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to fit the data for comparison with MCMC later\n",
    "# fit = Fit(dataset)\n",
    "# result = fit.run(optimize_opts={\"print_level\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate parameter correlations with MCMC\n",
    "\n",
    "Now let's analyse the simulated data.\n",
    "Here we just fit it again with the same model we had before as a starting point.\n",
    "The data that would be needed are the following: \n",
    "- counts cube, psf cube, exposure cube and background model\n",
    "\n",
    "Luckily all those maps are already in the Dataset object.\n",
    "\n",
    "We will need to define a Likelihood function and define priors on parameters.<br>\n",
    "Here we will assume a uniform prior reading the min, max parameters from the sky model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define priors\n",
    "\n",
    "This steps is a bit manual for the moment until we find a better API to define priors.<br>\n",
    "Note the you **need** to define priors for each parameter otherwise your walkers can explore uncharted territories (e.g. negative norms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "\n",
    "    # Define the free parameters and min, max values\n",
    "    parameters = dataset.models.parameters\n",
    "\n",
    "    # Setting the free/frozen parameters\n",
    "    parameters[\"norm\"].frozen = False\n",
    "\n",
    "    parameters[\"sigma\"].frozen = True\n",
    "    parameters[\"lon_0\"].frozen = True\n",
    "    parameters[\"lat_0\"].frozen = True\n",
    "    parameters[\"tilt\"].frozen = True\n",
    "\n",
    "    # Setting the priors\n",
    "    parameters[\"index\"].prior = GaussianPrior(mu=2.0, sigma=0.5)\n",
    "    parameters[\"norm\"].prior = GaussianPrior(mu=1.0, sigma=0.1)\n",
    "\n",
    "    # For uniform priors, choose how strong you want the prior to be\n",
    "    weight = 10\n",
    "\n",
    "    parameters[\"lambda_\"].prior = UniformPrior(min=1e-2, max=1, weight=weight)\n",
    "    parameters[\"amplitude\"].prior = UniformPrior(\n",
    "        min=3e-13, max=3e-11, weight=weight\n",
    "    )\n",
    "    parameters[\"sigma\"].prior = UniformPrior(min=0.01, max=0.5, weight=weight)\n",
    "\n",
    "    # Setting amplitude init values a bit offset to see evolution\n",
    "    # Here starting close to the real value\n",
    "    parameters[\"index\"].value = 1.5\n",
    "    parameters[\"amplitude\"].value = 5e-12\n",
    "    parameters[\"lambda_\"].value = 0.5\n",
    "    parameters[\"norm\"].value = 0.9\n",
    "\n",
    "\n",
    "init_model()\n",
    "\n",
    "print(dataset.models)\n",
    "print(\"stat =\", dataset.stat_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(pars, dataset):\n",
    "    \"\"\"\n",
    "    Estimate the likelihood of a model including prior on parameters.\n",
    "    Input :\n",
    "    pars : a list of parameters\n",
    "    dataset: a gammapy dataset\n",
    "    \"\"\"\n",
    "    # The MCMC sampler will evaluate the likelihood of the model given\n",
    "    # a set of parameters. We need to update the model parameters before\n",
    "    # evaluating the new likelihood value.\n",
    "    for value, parameter in zip(\n",
    "        pars, dataset.models.parameters.free_parameters\n",
    "    ):\n",
    "        parameter.value = value\n",
    "\n",
    "    # dataset.stat_sum returns Cash statistics values that is minimized\n",
    "    # emcee will maximisise the LogLikelihood so we need -dataset.stat_sum\n",
    "    total_lnprob = (\n",
    "        -dataset.stat_sum()\n",
    "    )  # stat_sum now includes stat + stat_priors\n",
    "\n",
    "    return total_lnprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "import logging\n",
    "\n",
    "\n",
    "nwalkers = 8\n",
    "nrun = 2000\n",
    "\n",
    "init_model()\n",
    "\n",
    "p0 = [free_par.value for free_par in dataset.models.parameters.free_parameters]\n",
    "labels = [\n",
    "    free_par.name for free_par in dataset.models.parameters.free_parameters\n",
    "]\n",
    "ndim = len(p0)\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "randomize_walkers = rng.normal(1, 0.03, size=(nwalkers, len(p0)))\n",
    "p0_walkers = (\n",
    "    np.tile(p0, [nwalkers, 1]) * randomize_walkers\n",
    ")  # init value for all walkers with slightly different values\n",
    "\n",
    "print(dataset.models.parameters.free_parameters[\"amplitude\"])\n",
    "print(dataset.models.parameters.free_parameters[\"lambda_\"])\n",
    "\n",
    "print(\"Initial values for walkers are : \", p0_walkers)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    lnprob,\n",
    "    args=[dataset],\n",
    ")\n",
    "\n",
    "log.info(f\"Free parameters: {labels}\")\n",
    "log.info(f\"Starting emcee sampling: nwalkers={nwalkers}, nrun={nrun}\")\n",
    "\n",
    "\n",
    "# Depending on your number of walkers, Nrun and dimensionality, this can take a while (> minutes)\n",
    "state = sampler.run_mcmc(\n",
    "    p0_walkers, nsteps=nrun, progress=True\n",
    ")  # to speedup the notebook\n",
    "\n",
    "samples1 = sampler.get_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "\n",
    "nwalkers = 8\n",
    "nrun = 1000\n",
    "\n",
    "init_model()\n",
    "\n",
    "p0 = [free_par.value for free_par in dataset.models.parameters.free_parameters]\n",
    "\n",
    "# Use the same starting points for both methods\n",
    "p0_walkers = (\n",
    "    np.tile(p0, [nwalkers, 1]) * randomize_walkers\n",
    ")  # init value for all walkers with slightly different values\n",
    "\n",
    "print(\"Initial values for walkers are : \", p0_walkers)\n",
    "\n",
    "\n",
    "sampler2 = zeus.EnsembleSampler(nwalkers, ndim, lnprob, args=[dataset])\n",
    "\n",
    "log.info(f\"Free parameters: {labels}\")\n",
    "log.info(f\"Starting Zeus MCMC sampling: nwalkers={nwalkers}, nrun={nrun}\")\n",
    "\n",
    "# Depending on your number of walkers, Nrun and dimensionality, this can take a while (> minutes)\n",
    "state = sampler2.run_mcmc(p0_walkers, nsteps=nrun, progress=True)\n",
    "samples2 = sampler2.get_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "The MCMC will return a sampler object containing the trace of all walkers.<br>\n",
    "The most important part is the chain attribute which is an array of shape:<br>\n",
    "_(nwalkers, nrun, nfreeparam)_\n",
    "\n",
    "The chain is then used to plot the trace of the walkers and estimate the burnin period (the time for the walkers to reach a stationary stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(labels), sharex=True, figsize=(10, 7))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.plot(samples1[:, :, idx], \"-k\", alpha=0.2)  # emcee\n",
    "    ax.plot(samples2[:, :, idx], \"-b\", alpha=0.2)  # Zeus MCMC\n",
    "    ax.set_ylabel(labels[idx])\n",
    "\n",
    "plt.xlabel(\"Nrun\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of both algorithms\n",
    "\n",
    "Note that the convergence is quite different between both MCMC algorithms.  \n",
    "`zeus-mcmc` was able to converge to a steady solution much faster than `emcee`. This means that you will burn less walkers steps and in the end you will have a better sampling of your posterior distributions. While `emcee` was faster to run you'll have to discard a larger fraction of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "\n",
    "nburn1 = 800\n",
    "nburn2 = 150\n",
    "\n",
    "print(\"Corner plot with emcee\")\n",
    "s = samples1[nburn1:, :, :].reshape((-1, len(labels)))\n",
    "corner(s, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Corner plot with Zeus MCMC\")\n",
    "s = samples2[nburn2:, :, :].reshape((-1, len(labels)))\n",
    "corner(s, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model dispersion\n",
    "\n",
    "Using the samples from the chain after the burn period, we can plot the different models compared to the truth model.  \n",
    "To do this we need to generate a spectral model for each parameter state in the sample.  \n",
    "The shaded area will represent the uncertainty band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.1, 100] * u.TeV\n",
    "nburn = 100\n",
    "nmodel = 100  # number of samples to draw\n",
    "\n",
    "samples = samples2\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for nwalk in range(0, nwalkers):\n",
    "    for n in range(nburn, nburn + nmodel):\n",
    "        pars = samples[n, nwalk, :]\n",
    "\n",
    "        # set model parameters\n",
    "        for i, free_par in enumerate(\n",
    "            dataset.models.parameters.free_parameters\n",
    "        ):\n",
    "            free_par.value = pars[i]\n",
    "        spectral_model = dataset.models[\"source\"].spectral_model\n",
    "\n",
    "        spectral_model.plot(\n",
    "            energy_bounds=(emin, emax),\n",
    "            ax=ax,\n",
    "            energy_power=2,\n",
    "            alpha=0.02,\n",
    "            color=\"grey\",\n",
    "        )\n",
    "\n",
    "\n",
    "sky_model_simu.spectral_model.plot(\n",
    "    energy_bounds=(emin, emax), energy_power=2, ax=ax, color=\"red\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun Zone\n",
    "\n",
    "Now that you have the sampler chain, you have in your hands the entire history of each walkers in the N-Dimensional parameter space. <br>\n",
    "You can for example trace the steps of each walker in any parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the trace of one walker in a given parameter space\n",
    "walkerid = 0\n",
    "parx = 0\n",
    "# Re-init the model to compare with the initial simulated parameters\n",
    "\n",
    "free_pars = dataset.models.parameters.free_parameters\n",
    "names = free_pars.names\n",
    "true_pars = models_true.parameters\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "\n",
    "    plt.plot(\n",
    "        samples1[:, walkerid, parx],\n",
    "        samples1[:, walkerid, i],\n",
    "        ls=\":\",\n",
    "        color=\"k\",\n",
    "        ms=1,\n",
    "        label=\"emcee\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        samples2[:, walkerid, parx],\n",
    "        samples2[:, walkerid, i],\n",
    "        ls=\":\",\n",
    "        color=\"blue\",\n",
    "        ms=1,\n",
    "        alpha=0.5,\n",
    "        label=\"Zeus\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        true_pars[parx].value,\n",
    "        true_pars[name].value,\n",
    "        \"+\",\n",
    "        color=\"red\",\n",
    "        markersize=15,\n",
    "        label=\"True value\",\n",
    "    )\n",
    "    plt.xlabel(names[parx])\n",
    "    plt.ylabel(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeVatrons in CTA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now itâ€™s your turn to play with this MCMC notebook. For example test the CTA performance to measure a cutoff at very high energies (100 TeV ?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
